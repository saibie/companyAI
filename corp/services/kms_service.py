from corp.models import CorporateMemory, Task
from ai_core.llm_gateway import OllamaClient
from pgvector.django import L2Distance
import os

# ì „ì—­ ì„ë² ë”© ëª¨ë¸ ì„¤ì • (Ollamaì— í•´ë‹¹ ëª¨ë¸ì´ pull ë˜ì–´ ìˆì–´ì•¼ í•¨)
EMBEDDING_MODEL = "nomic-embed-text" 

def get_embedding(text: str):
    """Ollamaë¥¼ í†µí•´ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    client = OllamaClient()
    try:
        response = client.embeddings(model=EMBEDDING_MODEL, prompt=text)
        return response.get('embedding')
    except Exception as e:
        print(f"âŒ Embedding Error: {e}")
        return []

def add_knowledge(owner, subject: str, content: str, source_task_id: int = None):
    """ì§€ì‹ì„ ë²¡í„°í™”í•˜ì—¬ ìœ„í‚¤ì— ì €ì¥í•©ë‹ˆë‹¤."""
    vector = get_embedding(f"{subject}\n{content}")
    if not vector:
        return None
        
    source_task = None
    if source_task_id:
        source_task = Task.objects.filter(id=source_task_id).first()

    memory = CorporateMemory.objects.create(
        owner=owner,
        subject=subject,
        content=content,
        embedding=vector,
        source_task=source_task
    )
    print(f"ğŸ“š [KMS] New knowledge added: {subject}")
    return memory

def search_wiki(query: str, top_k: int = 3):
    """ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ìœ„í‚¤ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    query_vector = get_embedding(query)
    if not query_vector:
        return []

    # L2 Distance(ìœ í´ë¦¬ë“œ ê±°ë¦¬)ë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰ (ê±°ë¦¬ê°€ ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬í•¨)
    results = CorporateMemory.objects.annotate(
        distance=L2Distance('embedding', query_vector)
    ).order_by('distance')[:top_k]
    
    return results